{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# NOTEBOOK 2: MATCHAI INFERENCE, 3 EXTRA MODELS, AND SCORING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 0: INSTALL REQUIRED PACKAGES\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 1: Install required libraries and import\n",
        "\n",
        "!pip install -q transformers sentence-transformers datasets\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from torch.nn.functional import cosine_similarity, softmax\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "9wBH99hzJQHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: LOAD PROCESSED DATASET & LABEL MAPPINGS\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 2: Load dataset from Hugging Face and rebuild numeric labels\n",
        "\n",
        "print(\"\\n=== STEP 1: Loading dataset from Hugging Face ===\")\n",
        "\n",
        "dataset = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
        "print(dataset)\n",
        "\n",
        "resume_col = \"resume_text\"\n",
        "jd_col = \"job_description_text\"\n",
        "label_col = \"label\"\n",
        "\n",
        "# Label normalization rules (same logic as Notebook 1)\n",
        "label_variations = {\n",
        "    'no fit': ['no fit','no_fit','no-fit','0','no','not fit','unfit'],\n",
        "    'potential fit': ['potential fit','potential_fit','potential-fit','1','potential','maybe','partial'],\n",
        "    'good fit': ['good fit','good_fit','good-fit','2','good','excellent','perfect','best'],\n",
        "}\n",
        "\n",
        "def normalize_label(raw):\n",
        "    s = str(raw).lower().strip()\n",
        "    if any(v in s for v in label_variations['no fit']):\n",
        "        return \"No Fit\"\n",
        "    if any(v in s for v in label_variations['potential fit']):\n",
        "        return \"Potential Fit\"\n",
        "    if any(v in s for v in label_variations['good fit']):\n",
        "        return \"Good Fit\"\n",
        "    return \"No Fit\"  # default fallback\n",
        "\n",
        "numeric_mapping = {\n",
        "    \"No Fit\": 0,\n",
        "    \"Potential Fit\": 1,\n",
        "    \"Good Fit\": 2\n",
        "}\n",
        "\n",
        "reverse_mapping = {v: k for k, v in numeric_mapping.items()}\n",
        "label_id2name = {\n",
        "    0: \"No Fit\",\n",
        "    1: \"Potential Fit\",\n",
        "    2: \"Good Fit\"\n",
        "}\n",
        "\n",
        "def add_numeric_label(example):\n",
        "    norm = normalize_label(example[label_col])\n",
        "    example[\"numeric_label\"] = numeric_mapping[norm]\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_numeric_label)\n",
        "\n",
        "print(\"\\nDataset with numeric_label added:\")\n",
        "print(dataset)\n",
        "\n",
        "train_split = dataset[\"train\"]\n",
        "test_split = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train size: {len(train_split)}, Test size: {len(test_split)}\")"
      ],
      "metadata": {
        "id": "7fEchnPHJkZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: LOAD FINE-TUNED CLASSIFIER FROM HUGGING FACE\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 3: Load fine-tuned classifier from Hugging Face\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"\\n=== STEP 2: Loading fine-tuned classifier from Hugging Face ===\")\n",
        "\n",
        "# üî¥ CHANGE THIS to your actual model on Hugging Face, e.g. \"ericachen/matchai-fit-classifier\"\n",
        "FINE_TUNED_MODEL_ID = \"your-username/matchai-fit-classifier\"  # <-- change this\n",
        "\n",
        "# 'device' should already be defined in Cell 1, but we guard just in case\n",
        "try:\n",
        "    device\n",
        "except NameError:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "clf_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_ID)\n",
        "clf_model = AutoModelForSequenceClassification.from_pretrained(FINE_TUNED_MODEL_ID)\n",
        "clf_model.to(device)\n",
        "clf_model.eval()\n",
        "\n",
        "print(\"Loaded classifier model from:\", FINE_TUNED_MODEL_ID)\n",
        "print(\"Number of labels:\", clf_model.config.num_labels)\n",
        "\n",
        "# label_id2name should have been defined in Cell 2:\n",
        "# label_id2name = {0: \"No Fit\", 1: \"Potential Fit\", 2: \"Good Fit\"}\n",
        "\n",
        "def predict_fit_label(jd_text: str, res_text: str):\n",
        "    \"\"\"\n",
        "    Use the fine-tuned classifier to predict suitability.\n",
        "    Returns label name and probabilities.\n",
        "    \"\"\"\n",
        "    combined = res_text + \" [SEP] \" + jd_text  # same concatenation used in training\n",
        "    inputs = clf_tokenizer(\n",
        "        combined,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clf_model(**inputs)\n",
        "        probs = softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    return {\n",
        "        \"label_id\": pred_id,\n",
        "        \"label_name\": label_id2name.get(pred_id, f\"Class {pred_id}\"),\n",
        "        \"probs\": probs.tolist()\n",
        "    }\n",
        "\n",
        "# Quick smoke test on one example\n",
        "example = test_split[0]\n",
        "test_pred = predict_fit_label(example[jd_col], example[resume_col])\n",
        "print(\"Test prediction:\", test_pred)"
      ],
      "metadata": {
        "id": "yTagbldFJzwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: SUMMARIZATION MODEL SELECTION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 4: Summarization Model Selection With Ranking\n",
        "\n",
        "print(\"\\n=== STEP 3: Summarization Model Selection With Ranking ===\")\n",
        "\n",
        "from transformers import pipeline\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Only load lightweight models for testing\n",
        "summarizer_candidates = {\n",
        "    \"t5_base\": \"t5-base\",\n",
        "    \"distilbart\": \"sshleifer/distilbart-cnn-12-6\",\n",
        "}\n",
        "\n",
        "# Prepare small evaluation sample (3 JDs, 3 resumes)\n",
        "sample_texts = []\n",
        "for i in range(3):\n",
        "    sample_texts.append(train_split[i][jd_col])\n",
        "    sample_texts.append(train_split[i][resume_col])\n",
        "\n",
        "def keyword_score(original, summary):\n",
        "    orig_tokens = set([w.lower() for w in original.split() if len(w) > 5])\n",
        "    sum_tokens  = set([w.lower() for w in summary.split() if len(w) > 5])\n",
        "    if not orig_tokens:\n",
        "        return 0\n",
        "    return len(orig_tokens.intersection(sum_tokens)) / len(orig_tokens)\n",
        "\n",
        "raw_results = []\n",
        "\n",
        "print(\"\\nRunning evaluation on candidate models...\\n\")\n",
        "\n",
        "for name, model_name in summarizer_candidates.items():\n",
        "    print(f\"--- Testing {name} ({model_name}) ---\")\n",
        "\n",
        "    summ = pipeline(\"summarization\", model=model_name, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "    comp_ratios, speeds, kw_scores = [], [], []\n",
        "\n",
        "    for text in sample_texts:\n",
        "        t = text[:2000]  # truncate long texts for stability\n",
        "\n",
        "        start = time.time()\n",
        "        summary = summ(t, max_length=150, min_length=40, do_sample=False)[0][\"summary_text\"]\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        comp = len(summary) / len(t)\n",
        "        kw = keyword_score(t, summary)\n",
        "\n",
        "        comp_ratios.append(comp)\n",
        "        speeds.append(elapsed)\n",
        "        kw_scores.append(kw)\n",
        "\n",
        "    raw_results.append({\n",
        "        \"name\": name,\n",
        "        \"model\": model_name,\n",
        "        \"avg_compression_ratio\": float(np.mean(comp_ratios)),\n",
        "        \"avg_keyword_score\": float(np.mean(kw_scores)),\n",
        "        \"avg_inference_time\": float(np.mean(speeds)),\n",
        "    })\n",
        "\n",
        "# Add BART-large in the ranking table as excluded\n",
        "raw_results.append({\n",
        "    \"name\": \"bart_large (excluded)\",\n",
        "    \"model\": \"facebook/bart-large-cnn\",\n",
        "    \"avg_compression_ratio\": None,\n",
        "    \"avg_keyword_score\": None,\n",
        "    \"avg_inference_time\": 20.0,\n",
        "    \"note\": \"Excluded due to memory failures and >20s inference\"\n",
        "})\n",
        "\n",
        "# Compute normalized scores for candidates only\n",
        "valid = [r for r in raw_results if r[\"avg_keyword_score\"] is not None]\n",
        "\n",
        "# Normalize metrics to 0‚Äì1 scale\n",
        "max_kw = max(r[\"avg_keyword_score\"] for r in valid)\n",
        "min_speed = min(r[\"avg_inference_time\"] for r in valid)\n",
        "max_comp = max(r[\"avg_compression_ratio\"] for r in valid)\n",
        "\n",
        "for r in valid:\n",
        "    r[\"keyword_norm\"] = r[\"avg_keyword_score\"] / max_kw\n",
        "    r[\"speed_norm\"] = min_speed / r[\"avg_inference_time\"]  # faster = higher score\n",
        "    r[\"compression_norm\"] = r[\"avg_compression_ratio\"] / max_comp\n",
        "\n",
        "    # Composite score\n",
        "    r[\"final_score\"] = (\n",
        "        0.4 * r[\"keyword_norm\"] +\n",
        "        0.3 * r[\"compression_norm\"] +\n",
        "        0.3 * r[\"speed_norm\"]\n",
        "    )\n",
        "\n",
        "# Sort by final score (highest first)\n",
        "ranked = sorted(valid, key=lambda x: x[\"final_score\"], reverse=True)\n",
        "\n",
        "print(\"\\nüèÜ Summarization Model Ranking (Composite Score)\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(f\"{'Rank':<5} {'Model':<25} {'Score':<10} {'Time(s)':<10}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "for i, r in enumerate(ranked, 1):\n",
        "    print(f\"{i:<5} {r['model']:<25} {r['final_score']:.4f}    {r['avg_inference_time']:.2f}\")\n",
        "\n",
        "print(\"\\nNote: 'facebook/bart-large-cnn' excluded for OOM and >20s inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaC7AfAeKh0E",
        "outputId": "7c7e96c2-f0f7-47c4-e074-a68ef52f2da0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STEP 3: Summarization Model Selection With Ranking ===\n",
            "\n",
            "Running evaluation on candidate models...\n",
            "\n",
            "--- Testing t5_base (t5-base) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing distilbart (sshleifer/distilbart-cnn-12-6) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üèÜ Summarization Model Ranking (Composite Score)\n",
            "------------------------------------------------------------\n",
            "Rank  Model                     Score      Time(s)   \n",
            "------------------------------------------------------------\n",
            "1     sshleifer/distilbart-cnn-12-6 1.0000    11.31\n",
            "2     t5-base                   0.9162    11.39\n",
            "\n",
            "Note: 'facebook/bart-large-cnn' excluded for OOM and >20s inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Use top-ranked summarizer + define summarize_text()\n",
        "\n",
        "\n",
        "from transformers import pipeline as hf_pipeline\n",
        "\n",
        "# Take the best summarizer from the ranking computed in Cell 4\n",
        "BEST_SUMMARIZER = ranked[0]  # 'ranked' is defined in Cell 4\n",
        "FINAL_SUMMARIZER_MODEL = BEST_SUMMARIZER[\"model\"]\n",
        "\n",
        "print(\"\\nSelected summarization model based on composite score:\")\n",
        "print(\"  Internal name :\", BEST_SUMMARIZER[\"name\"])\n",
        "print(\"  HF model ID   :\", FINAL_SUMMARIZER_MODEL)\n",
        "print(\"  Final score   :\", f\"{BEST_SUMMARIZER['final_score']:.4f}\")\n",
        "print(\"  Avg time (s)  :\", f\"{BEST_SUMMARIZER['avg_inference_time']:.2f}\")\n",
        "\n",
        "summarizer = hf_pipeline(\n",
        "    \"summarization\",\n",
        "    model=FINAL_SUMMARIZER_MODEL,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "def summarize_text(text: str, max_len: int = 150) -> str:\n",
        "    \"\"\"\n",
        "    Safely summarize text using the selected model:\n",
        "    - Handles None / empty input\n",
        "    - Truncates very long texts to avoid model/tokenizer issues\n",
        "    - Provides a graceful fallback if summarization fails\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    truncated_text = text[:2000]  # safety for very long resumes / JDs\n",
        "\n",
        "    try:\n",
        "        result = summarizer(\n",
        "            truncated_text,\n",
        "            max_length=max_len,\n",
        "            min_length=40,\n",
        "            do_sample=False,\n",
        "        )[0][\"summary_text\"]\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Summarization failed due to: {e}\")\n",
        "        # Fallback: return a truncated version of the original text\n",
        "        return truncated_text[:300]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhllwhRZKrEf",
        "outputId": "482f76a3-3fe2-4b49-bee6-9b5e5e3d6e67"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected summarization model based on composite score:\n",
            "  Internal name : distilbart\n",
            "  HF model ID   : sshleifer/distilbart-cnn-12-6\n",
            "  Final score   : 1.0000\n",
            "  Avg time (s)  : 11.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 4: SEMANTIC SIMILARITY MODEL SELECTION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 6: Semantic similarity model selection with ranking\n",
        "\n",
        "print(\"\\n=== STEP 4: Semantic similarity model selection with ranking ===\")\n",
        "\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "embedding_candidates = {\n",
        "    \"minilm\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"mpnet\": \"sentence-transformers/all-mpnet-base-v2\",\n",
        "    \"multiqa\": \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\",\n",
        "}\n",
        "\n",
        "# Build small positive (Good Fit) and negative (No Fit) sets from train split\n",
        "good_fit_pairs = []\n",
        "no_fit_pairs = []\n",
        "\n",
        "for ex in train_split:\n",
        "    label = ex[\"numeric_label\"]\n",
        "    if label == 2 and len(good_fit_pairs) < 15:\n",
        "        good_fit_pairs.append((ex[jd_col], ex[resume_col]))\n",
        "    if label == 0 and len(no_fit_pairs) < 15:\n",
        "        no_fit_pairs.append((ex[jd_col], ex[resume_col]))\n",
        "    if len(good_fit_pairs) >= 15 and len(no_fit_pairs) >= 15:\n",
        "        break\n",
        "\n",
        "print(\"Good Fit pairs:\", len(good_fit_pairs))\n",
        "print(\"No Fit pairs  :\", len(no_fit_pairs))\n",
        "\n",
        "sim_raw_results = []\n",
        "\n",
        "for name, model_name in embedding_candidates.items():\n",
        "    print(f\"\\n--- Testing embedding model: {name} ({model_name}) ---\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    sims_good, sims_no = [], []\n",
        "    start = time.time()\n",
        "\n",
        "    # Good Fit similarities\n",
        "    for jd_text, res_text in good_fit_pairs:\n",
        "        emb_jd = model.encode(jd_text, convert_to_tensor=True)\n",
        "        emb_res = model.encode(res_text, convert_to_tensor=True)\n",
        "        sims_good.append(cosine_similarity(emb_jd, emb_res, dim=0).item())\n",
        "\n",
        "    # No Fit similarities\n",
        "    for jd_text, res_text in no_fit_pairs:\n",
        "        emb_jd = model.encode(jd_text, convert_to_tensor=True)\n",
        "        emb_res = model.encode(res_text, convert_to_tensor=True)\n",
        "        sims_no.append(cosine_similarity(emb_jd, emb_res, dim=0).item())\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    avg_good = float(np.mean(sims_good))\n",
        "    avg_no = float(np.mean(sims_no))\n",
        "    gap = avg_good - avg_no  # we want Good Fit >> No Fit\n",
        "\n",
        "    sim_raw_results.append({\n",
        "        \"name\": name,\n",
        "        \"model\": model_name,\n",
        "        \"avg_good\": avg_good,\n",
        "        \"avg_no\": avg_no,\n",
        "        \"gap\": gap,\n",
        "        \"time_sec\": elapsed,\n",
        "    })\n",
        "\n",
        "print(\"\\nRaw similarity evaluation results:\")\n",
        "for r in sim_raw_results:\n",
        "    print(r)\n",
        "\n",
        "# Normalize gap and speed into a composite score\n",
        "valid_sim = sim_raw_results\n",
        "\n",
        "max_gap = max(r[\"gap\"] for r in valid_sim)\n",
        "min_time = min(r[\"time_sec\"] for r in valid_sim)\n",
        "\n",
        "for r in valid_sim:\n",
        "    # larger gap = better\n",
        "    r[\"gap_norm\"] = r[\"gap\"] / max_gap if max_gap > 0 else 0.0\n",
        "    # faster = better\n",
        "    r[\"speed_norm\"] = min_time / r[\"time_sec\"] if r[\"time_sec\"] > 0 else 0.0\n",
        "\n",
        "    # composite score (weight gap more heavily than speed)\n",
        "    r[\"final_score\"] = 0.7 * r[\"gap_norm\"] + 0.3 * r[\"speed_norm\"]\n",
        "\n",
        "# Rank models\n",
        "sim_ranked = sorted(valid_sim, key=lambda x: x[\"final_score\"], reverse=True)\n",
        "\n",
        "print(\"\\nüèÜ Similarity Model Ranking (Composite Score)\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(f\"{'Rank':<5} {'Model':<40} {'Score':<10} {'Gap':<10} {'Time(s)':<10}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "for i, r in enumerate(sim_ranked, 1):\n",
        "    print(f\"{i:<5} {r['model']:<40} {r['final_score']:.4f}   {r['gap']:.4f}   {r['time_sec']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3VzZl8bK_Xh",
        "outputId": "6410ec66-d3b0-472b-d8e9-dd850cfb7717"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STEP 4: Semantic similarity model selection with ranking ===\n",
            "Good Fit pairs: 15\n",
            "No Fit pairs  : 15\n",
            "\n",
            "--- Testing embedding model: minilm (sentence-transformers/all-MiniLM-L6-v2) ---\n",
            "\n",
            "--- Testing embedding model: mpnet (sentence-transformers/all-mpnet-base-v2) ---\n",
            "\n",
            "--- Testing embedding model: multiqa (sentence-transformers/multi-qa-MiniLM-L6-cos-v1) ---\n",
            "\n",
            "Raw similarity evaluation results:\n",
            "{'name': 'minilm', 'model': 'sentence-transformers/all-MiniLM-L6-v2', 'avg_good': 0.47122112711270653, 'avg_no': 0.33195153176784514, 'gap': 0.13926959534486139, 'time_sec': 8.818103313446045}\n",
            "{'name': 'mpnet', 'model': 'sentence-transformers/all-mpnet-base-v2', 'avg_good': 0.5821873227755229, 'avg_no': 0.4211725036303202, 'gap': 0.16101481914520266, 'time_sec': 85.70857048034668}\n",
            "{'name': 'multiqa', 'model': 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1', 'avg_good': 0.5364052474498748, 'avg_no': 0.3831758052110672, 'gap': 0.15322944223880763, 'time_sec': 15.846481323242188}\n",
            "\n",
            "üèÜ Similarity Model Ranking (Composite Score)\n",
            "------------------------------------------------------------\n",
            "Rank  Model                                    Score      Gap        Time(s)   \n",
            "------------------------------------------------------------\n",
            "1     sentence-transformers/all-MiniLM-L6-v2   0.9055   0.1393   8.82\n",
            "2     sentence-transformers/multi-qa-MiniLM-L6-cos-v1 0.8331   0.1532   15.85\n",
            "3     sentence-transformers/all-mpnet-base-v2  0.7309   0.1610   85.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Use best similarity model + define compute_similarity()\n",
        "\n",
        "\n",
        "BEST_SIM = sim_ranked[0]\n",
        "FINAL_EMBEDDING_MODEL = BEST_SIM[\"model\"]\n",
        "\n",
        "print(\"\\nSelected similarity model based on composite score:\")\n",
        "print(\"  Internal name :\", BEST_SIM[\"name\"])\n",
        "print(\"  HF model ID   :\", FINAL_EMBEDDING_MODEL)\n",
        "print(\"  Final score   :\", f\"{BEST_SIM['final_score']:.4f}\")\n",
        "print(\"  Gap (Good-No) :\", f\"{BEST_SIM['gap']:.4f}\")\n",
        "print(\"  Time (s)      :\", f\"{BEST_SIM['time_sec']:.2f}\")\n",
        "\n",
        "sim_model = SentenceTransformer(FINAL_EMBEDDING_MODEL)\n",
        "\n",
        "def compute_similarity(text1: str, text2: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute cosine similarity between two texts using the selected embedding model.\n",
        "    \"\"\"\n",
        "    emb1 = sim_model.encode(text1, convert_to_tensor=True)\n",
        "    emb2 = sim_model.encode(text2, convert_to_tensor=True)\n",
        "    sim = cosine_similarity(emb1, emb2, dim=0).item()\n",
        "    return float(sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXxmcTszLKkv",
        "outputId": "43e1d833-1714-41dc-9dc0-0eafb3978b27"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected similarity model based on composite score:\n",
            "  Internal name : minilm\n",
            "  HF model ID   : sentence-transformers/all-MiniLM-L6-v2\n",
            "  Final score   : 0.9055\n",
            "  Gap (Good-No) : 0.1393\n",
            "  Time (s)      : 8.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: NER MODEL SELECTION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 8: NER model selection with simple heuristic ranking\n",
        "\n",
        "print(\"\\n=== STEP 5: NER model selection with heuristic ranking ===\")\n",
        "\n",
        "from transformers import pipeline as hf_pipeline\n",
        "import time\n",
        "\n",
        "# Updated candidate list: removed invalid deepset/roberta-base-medium-ner\n",
        "ner_candidates = {\n",
        "    \"bert_ner\": \"dslim/bert-base-NER\",                      # English, widely used\n",
        "    \"xlm_ner\": \"Davlan/xlm-roberta-base-ner-hrl\",           # multilingual high-resource\n",
        "    \"multi_ner\": \"Babelscape/wikineural-multilingual-ner\",  # multilingual NER\n",
        "}\n",
        "\n",
        "# Take a few sample resumes for heuristic comparison\n",
        "sample_resumes = [x[resume_col] for x in train_split.select(range(min(5, len(train_split))))]\n",
        "\n",
        "ner_raw_results = []\n",
        "\n",
        "for name, model_name in ner_candidates.items():\n",
        "    print(f\"\\n--- Testing NER model: {name} ({model_name}) ---\")\n",
        "    try:\n",
        "        ner_pipe = hf_pipeline(\"ner\", model=model_name, grouped_entities=True)\n",
        "    except Exception as e:\n",
        "        print(f\"  [SKIP] Failed to load {model_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    total_org, total_entities, total_time = 0, 0, 0.0\n",
        "\n",
        "    for res in sample_resumes:\n",
        "        text = res[:1000]  # truncate for speed\n",
        "        start = time.time()\n",
        "        ents = ner_pipe(text)\n",
        "        elapsed = time.time() - start\n",
        "        total_time += elapsed\n",
        "\n",
        "        total_entities += len(ents)\n",
        "        total_org += sum(1 for e in ents if e.get(\"entity_group\") == \"ORG\")\n",
        "\n",
        "    avg_org = total_org / len(sample_resumes)\n",
        "    avg_ents = total_entities / len(sample_resumes)\n",
        "    avg_time = total_time / len(sample_resumes)\n",
        "\n",
        "    ner_raw_results.append({\n",
        "        \"name\": name,\n",
        "        \"model\": model_name,\n",
        "        \"avg_org\": avg_org,          # how many ORG entities we detect on average\n",
        "        \"avg_entities\": avg_ents,    # total entities\n",
        "        \"avg_time\": avg_time,\n",
        "    })\n",
        "    print(f\"  Avg ORG entities: {avg_org:.2f}, Avg total entities: {avg_ents:.2f}, Avg time: {avg_time:.2f}s\")\n",
        "\n",
        "if not ner_raw_results:\n",
        "    raise RuntimeError(\"All NER models failed to load. Please check internet or model IDs.\")\n",
        "\n",
        "# Normalize ORG count and time ‚Üí composite score\n",
        "valid_ner = ner_raw_results\n",
        "\n",
        "max_org = max(r[\"avg_org\"] for r in valid_ner)\n",
        "min_time_ner = min(r[\"avg_time\"] for r in valid_ner)\n",
        "\n",
        "for r in valid_ner:\n",
        "    r[\"org_norm\"] = r[\"avg_org\"] / max_org if max_org > 0 else 0.0\n",
        "    r[\"speed_norm\"] = min_time_ner / r[\"avg_time\"] if r[\"avg_time\"] > 0 else 0.0\n",
        "\n",
        "    # composite: more ORG, faster speed\n",
        "    r[\"final_score\"] = 0.7 * r[\"org_norm\"] + 0.3 * r[\"speed_norm\"]\n",
        "\n",
        "# Rank NER models\n",
        "ner_ranked = sorted(valid_ner, key=lambda x: x[\"final_score\"], reverse=True)\n",
        "\n",
        "print(\"\\nüèÜ NER Model Ranking (Composite Score)\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "print(f\"{'Rank':<5} {'Model':<40} {'Score':<10} {'ORG':<10} {'Time(s)':<10}\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "for i, r in enumerate(ner_ranked, 1):\n",
        "    print(f\"{i:<5} {r['model']:<40} {r['final_score']:.4f}   {r['avg_org']:.2f}   {r['avg_time']:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vr0v_pLW1w",
        "outputId": "d3ff5cde-3cfa-4cdf-926f-b5c00fd6598d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STEP 5: NER model selection with heuristic ranking ===\n",
            "\n",
            "--- Testing NER model: bert_ner (dslim/bert-base-NER) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Avg ORG entities: 3.00, Avg total entities: 7.20, Avg time: 1.59s\n",
            "\n",
            "--- Testing NER model: xlm_ner (Davlan/xlm-roberta-base-ner-hrl) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Avg ORG entities: 1.80, Avg total entities: 5.40, Avg time: 2.26s\n",
            "\n",
            "--- Testing NER model: multi_ner (Babelscape/wikineural-multilingual-ner) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Avg ORG entities: 3.60, Avg total entities: 14.60, Avg time: 2.12s\n",
            "\n",
            "üèÜ NER Model Ranking (Composite Score)\n",
            "------------------------------------------------------------\n",
            "Rank  Model                                    Score      ORG        Time(s)   \n",
            "------------------------------------------------------------\n",
            "1     Babelscape/wikineural-multilingual-ner   0.9249   3.60   2.12\n",
            "2     dslim/bert-base-NER                      0.8833   3.00   1.59\n",
            "3     Davlan/xlm-roberta-base-ner-hrl          0.5611   1.80   2.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Use best NER model + define extract_entities()\n",
        "\n",
        "\n",
        "BEST_NER = ner_ranked[0]\n",
        "FINAL_NER_MODEL = BEST_NER[\"model\"]\n",
        "\n",
        "print(\"\\nSelected NER model based on composite score:\")\n",
        "print(\"  Internal name :\", BEST_NER[\"name\"])\n",
        "print(\"  HF model ID   :\", FINAL_NER_MODEL)\n",
        "print(\"  Final score   :\", f\"{BEST_NER['final_score']:.4f}\")\n",
        "print(\"  Avg ORG count :\", f\"{BEST_NER['avg_org']:.2f}\")\n",
        "print(\"  Avg time (s)  :\", f\"{BEST_NER['avg_time']:.2f}\")\n",
        "\n",
        "ner = hf_pipeline(\"ner\", model=FINAL_NER_MODEL, grouped_entities=True)\n",
        "\n",
        "def extract_entities(text: str):\n",
        "    \"\"\"\n",
        "    Extract main entities from resume text:\n",
        "    Returns dict with ORG, PER, LOC lists.\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"ORG\": [], \"PER\": [], \"LOC\": []}\n",
        "    ents = ner(text[:1000])\n",
        "    result = {\"ORG\": [], \"PER\": [], \"LOC\": []}\n",
        "    for e in ents:\n",
        "        label = e.get(\"entity_group\")\n",
        "        word = e.get(\"word\", \"\").strip()\n",
        "        if label in result and word:\n",
        "            result[label].append(word)\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPPRxPeqLdTa",
        "outputId": "b970cf62-bd63-4f61-beb8-55951164e478"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected NER model based on composite score:\n",
            "  Internal name : multi_ner\n",
            "  HF model ID   : Babelscape/wikineural-multilingual-ner\n",
            "  Final score   : 0.9249\n",
            "  Avg ORG count : 3.60\n",
            "  Avg time (s)  : 2.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: PROCESSING HELPERS (JD & RESUME)\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 10: Processing helpers for JD and resume\n",
        "\n",
        "print(\"\\n=== STEP 6: Defining JD & resume processing helpers ===\")\n",
        "\n",
        "def process_job_description(jd_text: str):\n",
        "    summary = summarize_text(jd_text)\n",
        "    # simple keyword extraction: unique words longer than 4 chars\n",
        "    keywords = list({\n",
        "        w.lower() for w in summary.split() if len(w) > 4\n",
        "    })\n",
        "    return {\n",
        "        \"raw\": jd_text,\n",
        "        \"summary\": summary,\n",
        "        \"keywords\": keywords,\n",
        "    }\n",
        "\n",
        "def process_resume(res_text: str):\n",
        "    summary = summarize_text(res_text)\n",
        "    entities = extract_entities(res_text)\n",
        "    return {\n",
        "        \"raw\": res_text,\n",
        "        \"summary\": summary,\n",
        "        \"entities\": entities,\n",
        "    }\n",
        "\n",
        "print(\"Helpers process_job_description() and process_resume() are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hExJHrtDLiYg",
        "outputId": "acd3e387-e3d6-4b05-df3a-38eee7f9d480"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STEP 6: Defining JD & resume processing helpers ===\n",
            "Helpers process_job_description() and process_resume() are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 7: FINAL SCORING FUNCTION COMBINING 4 MODELS\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 11: Final MatchAI scoring function (4-model pipeline)\n",
        "\n",
        "print(\"\\n=== STEP 7: Building final MatchAI scoring function ===\")\n",
        "\n",
        "# Default weights (later can be HR-adjustable in Streamlit)\n",
        "WEIGHTS = {\n",
        "    \"classifier\": 0.5,   # probability of Good Fit\n",
        "    \"similarity\": 0.3,   # semantic similarity on summaries\n",
        "    \"keywords\": 0.2,     # JD keyword coverage in resume summary\n",
        "}\n",
        "\n",
        "def keyword_match_score(jd_keywords, resume_summary: str) -> float:\n",
        "    \"\"\"\n",
        "    Simple keyword coverage score:\n",
        "    proportion of JD keywords that appear in the resume summary.\n",
        "    \"\"\"\n",
        "    resume_words = set(w.lower() for w in resume_summary.split())\n",
        "    if not jd_keywords:\n",
        "        return 0.0\n",
        "    hits = sum(1 for kw in jd_keywords if kw in resume_words)\n",
        "    return hits / len(jd_keywords)\n",
        "\n",
        "def evaluate_candidate(jd_text: str, res_text: str):\n",
        "    \"\"\"\n",
        "    Full evaluation pipeline:\n",
        "    - Summarize JD and resume (summarization model selected in STEP 3)\n",
        "    - Extract entities from resume (NER model selected in STEP 5)\n",
        "    - Compute semantic similarity between summaries (embedding model from STEP 4)\n",
        "    - Run fine-tuned classifier (Notebook 1 model, loaded in STEP 2)\n",
        "    - Compute keyword match between JD summary keywords and resume summary\n",
        "    - Combine into final weighted suitability score\n",
        "    \"\"\"\n",
        "    # Process JD and resume\n",
        "    jd = process_job_description(jd_text)\n",
        "    res = process_resume(res_text)\n",
        "\n",
        "    # Similarity between summaries\n",
        "    sim_raw = compute_similarity(jd[\"summary\"], res[\"summary\"])\n",
        "    # Normalize similarity from [-1, 1] to ~[0, 1]\n",
        "    sim_norm = (sim_raw + 1) / 2 if sim_raw < 1 else min(sim_raw, 1.0)\n",
        "\n",
        "    # Keyword match\n",
        "    kw_score = keyword_match_score(jd[\"keywords\"], res[\"summary\"])\n",
        "\n",
        "    # Classifier prediction (uses fine-tuned model)\n",
        "    fit = predict_fit_label(jd_text, res_text)\n",
        "    prob_good_fit = fit[\"probs\"][2]  # probability of Good Fit (label id 2)\n",
        "\n",
        "    # Final weighted score\n",
        "    final_score = (\n",
        "        WEIGHTS[\"classifier\"] * prob_good_fit +\n",
        "        WEIGHTS[\"similarity\"] * sim_norm +\n",
        "        WEIGHTS[\"keywords\"] * kw_score\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"jd\": jd,\n",
        "        \"resume\": res,\n",
        "        \"similarity_raw\": sim_raw,\n",
        "        \"similarity\": sim_norm,\n",
        "        \"keyword_score\": kw_score,\n",
        "        \"fit\": fit,\n",
        "        \"prob_good_fit\": prob_good_fit,\n",
        "        \"final_score\": float(final_score),\n",
        "    }\n",
        "\n",
        "print(\"evaluate_candidate(jd_text, res_text) is ready.\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXTRA: Generate human-readable candidate highlights\n",
        "# ============================================================================\n",
        "\n",
        "def generate_candidate_highlights(result: dict, thresholds: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Generate a short, recruiter-friendly highlight summary for one candidate,\n",
        "    based on the evaluation result from evaluate_candidate().\n",
        "\n",
        "    Input:\n",
        "        result:   dict returned by evaluate_candidate(jd_text, res_text)\n",
        "        thresholds (optional): override default thresholds\n",
        "\n",
        "    Output:\n",
        "        {\n",
        "            \"highlights\": [list of bullet strings],\n",
        "            \"summary\": single_string_summary\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Default thresholds (tunable or later exposed in UI)\n",
        "    default_thresholds = {\n",
        "        \"final_score_strong\": 0.80,\n",
        "        \"final_score_good\":  0.70,\n",
        "        \"prob_good_fit_high\": 0.80,\n",
        "        \"similarity_high\":    0.70,\n",
        "        \"keyword_high\":       0.60,\n",
        "        \"org_count_high\":     3,\n",
        "    }\n",
        "    if thresholds is not None:\n",
        "        default_thresholds.update(thresholds)\n",
        "    t = default_thresholds\n",
        "\n",
        "    highlights = []\n",
        "\n",
        "    final_score = result.get(\"final_score\", 0.0)\n",
        "    prob_good_fit = result.get(\"prob_good_fit\", 0.0)\n",
        "    similarity = result.get(\"similarity\", 0.0)\n",
        "    keyword_score = result.get(\"keyword_score\", 0.0)\n",
        "    fit_label = result.get(\"fit\", {}).get(\"label_name\", \"Unknown\")\n",
        "    entities = result.get(\"resume\", {}).get(\"entities\", {})\n",
        "    orgs = entities.get(\"ORG\", [])\n",
        "\n",
        "    # 1. Overall fit strength (final score + classifier)\n",
        "    if final_score >= t[\"final_score_strong\"] and prob_good_fit >= t[\"prob_good_fit_high\"]:\n",
        "        highlights.append(\n",
        "            f\"Very strong overall match (final score {final_score:.2f}, Good Fit probability {prob_good_fit:.2f}).\"\n",
        "        )\n",
        "    elif final_score >= t[\"final_score_good\"]:\n",
        "        highlights.append(\n",
        "            f\"Solid match (final score {final_score:.2f}) with classifier label: {fit_label}.\"\n",
        "        )\n",
        "    elif fit_label == \"Good Fit\":\n",
        "        highlights.append(\n",
        "            f\"Classifier marks this candidate as '{fit_label}', although overall score is moderate ({final_score:.2f}).\"\n",
        "        )\n",
        "\n",
        "    # 2. JD‚Äìresume alignment via semantic similarity\n",
        "    if similarity >= t[\"similarity_high\"]:\n",
        "        highlights.append(\n",
        "            f\"Resume narrative is highly aligned with the job description (semantic similarity {similarity:.2f}).\"\n",
        "        )\n",
        "\n",
        "    # 3. Keyword coverage from JD in resume summary\n",
        "    if keyword_score >= t[\"keyword_high\"]:\n",
        "        highlights.append(\n",
        "            f\"Strong coverage of key requirements mentioned in the job description (keyword match {keyword_score:.2f}).\"\n",
        "        )\n",
        "\n",
        "    # 4. Company background (ORG entities)\n",
        "    if len(orgs) >= t[\"org_count_high\"]:\n",
        "        unique_orgs = list(dict.fromkeys(orgs))  # preserve order, remove duplicates\n",
        "        top_orgs = \", \".join(unique_orgs[:3])\n",
        "        highlights.append(\n",
        "            f\"Rich company background, including experience with: {top_orgs}.\"\n",
        "        )\n",
        "\n",
        "    # 5. If nothing stands out, provide a neutral note\n",
        "    if not highlights:\n",
        "        highlights.append(\n",
        "            \"No particular standout factors detected; scores are moderate across fit, similarity, and keyword coverage.\"\n",
        "        )\n",
        "\n",
        "    # Build a single-line summary (for UI or report)\n",
        "    if len(highlights) == 1:\n",
        "        summary = highlights[0]\n",
        "    else:\n",
        "        # Take the strongest 1‚Äì2 points for a concise summary\n",
        "        summary = \" \".join(highlights[:2])\n",
        "\n",
        "    return {\n",
        "        \"highlights\": highlights,\n",
        "        \"summary\": summary\n",
        "    }"
      ],
      "metadata": {
        "id": "hP5_J1mhLv9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 8: SANITY TEST ON A FEW CANDIDATES\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 12: Sanity test on a few candidates\n",
        "\n",
        "print(\"\\n=== STEP 8: Sanity test on a few candidates ===\")\n",
        "\n",
        "if len(test_split) >= 4:\n",
        "    jd_example = test_split[0][jd_col]\n",
        "    print(\"\\nUsing JD from test[0] for demo.\")\n",
        "\n",
        "    candidate_resumes = [\n",
        "        test_split[i][resume_col] for i in range(1, 4)\n",
        "    ]\n",
        "\n",
        "    for idx, res_text in enumerate(candidate_resumes, start=1):\n",
        "        result = evaluate_candidate(jd_example, res_text)\n",
        "        highlights = generate_candidate_highlights(result)\n",
        "\n",
        "        print(f\"\\n--- Candidate {idx} ---\")\n",
        "        print(\"Fit label      :\", result[\"fit\"][\"label_name\"])\n",
        "        print(\"Prob Good Fit  :\", f\"{result['prob_good_fit']:.3f}\")\n",
        "        print(\"Final score    :\", f\"{result['final_score']:.3f}\")\n",
        "        print(\"Similarity     :\", f\"{result['similarity']:.3f}\")\n",
        "        print(\"Keyword score  :\", f\"{result['keyword_score']:.3f}\")\n",
        "        print(\"ORG entities   :\", result[\"resume\"][\"entities\"][\"ORG\"][:3])\n",
        "        print(\"Highlight summary:\", highlights[\"summary\"])\n",
        "        print(\"Full highlights:\")\n",
        "        for h in highlights[\"highlights\"]:\n",
        "            print(\" -\", h)\n",
        "else:\n",
        "    print(\"Not enough test samples for demo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "NmZxgAXkL6tF",
        "outputId": "3724ef25-d223-4a3a-eeb7-4813340e5711"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== STEP 8: Sanity test on a few candidates ===\n",
            "\n",
            "Using JD from test[0] for demo.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predict_fit_label' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1779486862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_resumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mhighlights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_candidate_highlights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1675438860.py\u001b[0m in \u001b[0;36mevaluate_candidate\u001b[0;34m(jd_text, res_text)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Classifier prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fit_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjd_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mprob_good_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"probs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# probability of Good Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_fit_label' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 9: SAVE MATCHAI CONFIG FOR STREAMLIT / DEPLOYMENT\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# Cell 13: Save MatchAI configuration for reuse\n",
        "\n",
        "print(\"\\n=== STEP 9: Saving MatchAI configuration ===\")\n",
        "\n",
        "matchai_config = {\n",
        "    \"fine_tuned_model_id\": FINE_TUNED_MODEL_ID,\n",
        "    \"summarization_model\": FINAL_SUMMARIZER_MODEL,\n",
        "    \"embedding_model\": FINAL_EMBEDDING_MODEL,\n",
        "    \"ner_model\": FINAL_NER_MODEL,\n",
        "    \"weights\": WEIGHTS,\n",
        "    \"label_id2name\": label_id2name,\n",
        "}\n",
        "\n",
        "with open(\"matchai_config.json\", \"w\") as f:\n",
        "    json.dump(matchai_config, f, indent=2)\n",
        "\n",
        "print(\"Saved matchai_config.json with model choices and weights.\")\n",
        "print(\"\\n=== Notebook 2 completed: 4-model MatchAI pipeline is ready. ===\")"
      ],
      "metadata": {
        "id": "D22WfkwwL-d7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
